{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "import json \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Sets - Abalone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalones_df = pd.read_csv('COMP472-A1-datasets/abalone.csv')\n",
    "# print(penguins_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print csv file info\n",
    "print(abalones_df.info()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # label encoding string values to int\n",
    "le = preprocessing.LabelEncoder()\n",
    "abalones_df['Type'] = le.fit_transform(abalones_df['Type'])\n",
    "\n",
    "print(abalones_df.info()) \n",
    "print(abalones_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the features into 1-hot vectors\n",
    "# abalones_df = pd.get_dummies(abalones_df, columns=['Type'], prefix='Type', dtype='int64')\n",
    "# print(abalones_df.info()) \n",
    "# print(abalones_df.head())\n",
    "\n",
    "# Note we've left this commented out to not affect columns of the table being used for the rest of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pie chart based on output class species\n",
    "total = abalones_df['Type'].count()\n",
    "\n",
    "num_females = abalones_df[abalones_df['Type'] == 0]['Type'].count()\n",
    "percent_females= num_females / total * 100\n",
    "\n",
    "num_infants = abalones_df[abalones_df['Type'] == 1]['Type'].count()\n",
    "percent_infants = num_infants / total * 100\n",
    "\n",
    "num_males = abalones_df[abalones_df['Type'] == 2]['Type'].count()\n",
    "percent_males = num_males / total * 100\n",
    "\n",
    "species_percentages = [percent_females, percent_males, percent_infants]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.pie(species_percentages, labels=['Females', 'Males', 'Infants'], autopct='%1.1f%%')\n",
    "plt.title(\"Percentange of Each Abalone Type\")\n",
    "plt.savefig('ablone_types_pie_chart.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Note: dataset fairly balanced in terms of abalone types\n",
    "\n",
    "# balanced dataset : accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing sets\n",
    "# default split is 25% testing, 75% training\n",
    "# data is shuffled by default, but no seeding applied \n",
    "\n",
    "X, y = [abalones_df.drop('Type', axis=1), abalones_df['Type']]\n",
    "\n",
    "X_train_set, X_test_set, y_train_set, y_test_set = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Performance Measures File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abalone-performance.txt', 'w') as f:\n",
    "    f.write('Performance Measures \\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abalone-performance_5_runs.txt', 'w') as f:\n",
    "    f.write('Performance Measures \\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base-DT\n",
    "=> a decision tree with the default parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default parameter for criterion = Gini impurity \n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "\n",
    "dtc.fit(X_train_set, y_train_set)\n",
    "tree.plot_tree(dtc, max_depth=6)\n",
    "\n",
    "dot_data = tree.export_graphviz(dtc, out_file=None,\n",
    "    feature_names= ['LongestShell', 'Diameter', 'Height', 'WholeWeight', 'ShuckedWeight', 'VisceraWeight', 'ShellWeight', 'Rings'],\n",
    "    class_names=['F','M','I'],\n",
    "    filled=True, \n",
    "    rounded=True,\n",
    "    max_depth=6,)\n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "graph.render(\"abalone_types_base_dt\")   # save to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = dtc.predict(X_test_set)\n",
    "\n",
    "print(X_test_set)\n",
    "print(\"Predicted output: \", le.inverse_transform(y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base-DT Performance Measures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abalone-performance.txt', 'a') as f:\n",
    "    f.write('**********************************************************\\n\\n(A) Base-DT Performance Measures \\n\\n')\n",
    "    f.write('Hyperparameters: \\n \\t- criterion: Gini impurity \\n\\t- max_depth: 6 \\n\\t- min_samples_split: 2 \\n\\n')\n",
    "    f.write('\\n(B) Confusion Matrix: \\n')\n",
    "    f.write(str(confusion_matrix(y_test_set, y_predict)))\n",
    "    f.write('\\n\\n(C - D) Classification Report: \\n')\n",
    "    f.write(str(classification_report(y_test_set, y_predict)))\n",
    "    f.write('\\n(C - D) Accuracy Score: ')\n",
    "    f.write(str(accuracy_score(y_test_set, y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the Base-DT code above 5 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dt_accuracy_scores = []\n",
    "base_dt_f1_scores_macro_avgs = []\n",
    "base_dt_f1_scores_weighted_avgs = []\n",
    "\n",
    "for i in range(5):\n",
    "    # default parameter for criterion = Gini impurity \n",
    "    dtc = tree.DecisionTreeClassifier()\n",
    "\n",
    "    dtc.fit(X_train_set, y_train_set)\n",
    "    tree.plot_tree(dtc, max_depth=6)\n",
    "\n",
    "    dot_data = tree.export_graphviz(dtc, out_file=None,\n",
    "        feature_names= ['LongestShell', 'Diameter', 'Height', 'WholeWeight', 'ShuckedWeight', 'VisceraWeight', 'ShellWeight', 'Rings'],\n",
    "        class_names=['F','M','I'],\n",
    "        filled=True, \n",
    "        rounded=True,\n",
    "        max_depth=6,)\n",
    "    graph = graphviz.Source(dot_data) \n",
    "\n",
    "    # graph.render(\"abalone_types_base_dt\")   # save to pdf\n",
    "\n",
    "    y_predict = dtc.predict(X_test_set)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_set, y_predict)\n",
    "    base_dt_accuracy_scores.append(accuracy)\n",
    "    \n",
    "    f1_score_macro_avg = f1_score(y_test_set, y_predict, average='macro')\n",
    "    base_dt_f1_scores_macro_avgs.append(f1_score_macro_avg)\n",
    "    \n",
    "    f1_score_weighted_avg = f1_score(y_test_set, y_predict, average='weighted')\n",
    "    base_dt_f1_scores_weighted_avgs.append(f1_score_weighted_avg)\n",
    "    \n",
    "    print(X_test_set)\n",
    "    print(\"Predicted output: \", le.inverse_transform(y_predict))\n",
    "    with open('abalone-performance_5_runs.txt', 'a') as f:\n",
    "        f.write(f'\\n\\n**********************************************************\\n\\nRUN {i + 1} \\n\\n(A) Base-DT Performance Measures \\n\\n')\n",
    "        f.write('Hyperparameters: \\n \\t- criterion: Gini impurity \\n\\t- max_depth: None \\n\\t- min_samples_split: 2 \\n\\n')\n",
    "        f.write('\\n(B) Confusion Matrix: \\n')\n",
    "        f.write(str(confusion_matrix(y_test_set, y_predict)))\n",
    "        f.write('\\n\\n(C - D) Classification Report: \\n')\n",
    "        f.write(str(classification_report(y_test_set, y_predict)))\n",
    "        f.write('\\n(C - D) Accuracy Score: ')\n",
    "        f.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-DT\n",
    "=> a Decision Tree found using a gridsearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decision tree model\n",
    "dtc_top_dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "# define hyperparameters to test for best model\n",
    "hyperparameters = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 6, 2],\n",
    "    'min_samples_split': [35, 150, 300],\n",
    "}\n",
    "\n",
    "# use grid search to find best hyperparameters\n",
    "gs = GridSearchCV(dtc_top_dt, hyperparameters, verbose=1)\n",
    "gs.fit(X_train_set, y_train_set)\n",
    "best_hyperparameters = gs.best_params_\n",
    "\n",
    "print('Best hyperparameters:\\n', best_hyperparameters)\n",
    "\n",
    "# create decision tree model with best hyperparameters\n",
    "dtc_top_dt_best_params = tree.DecisionTreeClassifier(**best_hyperparameters)\n",
    "# train model with best hyperparameters\n",
    "dtc_top_dt_best_params.fit(X_train_set, y_train_set)\n",
    "\n",
    "# plot decision tree\n",
    "tree.plot_tree(dtc_top_dt_best_params)\n",
    "\n",
    "dot_data = tree.export_graphviz(dtc_top_dt_best_params, out_file=None,\n",
    "    feature_names= ['LongestShell', 'Diameter', 'Height', 'WholeWeight', 'ShuckedWeight', 'VisceraWeight', 'ShellWeight', 'Rings'],\n",
    "    class_names=['F','M','I'],\n",
    "    filled=True, \n",
    "    rounded=True,\n",
    ")\n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "graph.render(\"abalone_types_top_dt\")   # save to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = dtc_top_dt_best_params.predict(X_test_set)\n",
    "\n",
    "print(X_test_set)\n",
    "print(\"Predicted output: \", le.inverse_transform(y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-DT Performance Measures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abalone-performance.txt', 'a') as f:\n",
    "    f.write('\\n\\n\\n**********************************************************\\n\\n(A) Top-DT Performance Measures \\n\\n')\n",
    "    f.write(\"Hyperparameters: \" + str(json.dump(best_hyperparameters, f)) + \"\\n\\n\")\n",
    "    f.write('\\n(B) Confusion Matrix: \\n')\n",
    "    f.write(str(confusion_matrix(y_test_set, y_predict)))\n",
    "    f.write('\\n\\n(C - D) Classification Report: \\n')\n",
    "    f.write(str(classification_report(y_test_set, y_predict)))\n",
    "    f.write('\\n(C - D) Accuracy Score: ')\n",
    "    f.write(str(accuracy_score(y_test_set, y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the Top-DT code above 5 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dt_accuracy_scores = []\n",
    "top_dt_f1_scores_macro_avgs = []\n",
    "top_dt_f1_scores_weighted_avgs = []\n",
    "\n",
    "for i in range(5):\n",
    "    # create decision tree model\n",
    "    dtc_top_dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "    # define hyperparameters to test for best model\n",
    "    hyperparameters = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 6, 2],\n",
    "        'min_samples_split': [35, 150, 300],\n",
    "    }\n",
    "\n",
    "    # use grid search to find best hyperparameters\n",
    "    gs = GridSearchCV(dtc_top_dt, hyperparameters, verbose=1)\n",
    "    gs.fit(X_train_set, y_train_set)\n",
    "    best_hyperparameters = gs.best_params_\n",
    "\n",
    "    print('Best hyperparameters:\\n', best_hyperparameters)\n",
    "\n",
    "    # create decision tree model with best hyperparameters\n",
    "    dtc_top_dt_best_params = tree.DecisionTreeClassifier(**best_hyperparameters)\n",
    "    # train model with best hyperparameters\n",
    "    dtc_top_dt_best_params.fit(X_train_set, y_train_set)\n",
    "\n",
    "    # plot decision tree\n",
    "    tree.plot_tree(dtc_top_dt_best_params)\n",
    "\n",
    "    dot_data = tree.export_graphviz(dtc_top_dt_best_params, out_file=None,\n",
    "        feature_names= ['LongestShell', 'Diameter', 'Height', 'WholeWeight', 'ShuckedWeight', 'VisceraWeight', 'ShellWeight', 'Rings'],\n",
    "        class_names=['F','M','I'],\n",
    "        filled=True, \n",
    "        rounded=True,\n",
    "    )\n",
    "    graph = graphviz.Source(dot_data) \n",
    "\n",
    "    # graph.render(\"abalone_types_top_dt\")   # save to pdf\n",
    "    \n",
    "    y_predict = dtc_top_dt_best_params.predict(X_test_set)\n",
    "\n",
    "    print(X_test_set)\n",
    "    print(\"Predicted output: \", le.inverse_transform(y_predict))\n",
    "\n",
    "    accuracy = accuracy_score(y_test_set, y_predict)\n",
    "    top_dt_accuracy_scores.append(accuracy)\n",
    "    \n",
    "    f1_score_macro_avg = f1_score(y_test_set, y_predict, average='macro')\n",
    "    top_dt_f1_scores_macro_avgs.append(f1_score_macro_avg)\n",
    "    \n",
    "    f1_score_weighted_avg = f1_score(y_test_set, y_predict, average='weighted')\n",
    "    top_dt_f1_scores_weighted_avgs.append(f1_score_weighted_avg)\n",
    "    \n",
    "    print(X_test_set)\n",
    "    print(\"Predicted output: \", le.inverse_transform(y_predict))\n",
    "    with open('abalone-performance_5_runs.txt', 'a') as f:\n",
    "        f.write(f'\\n\\n\\n**********************************************************\\n\\nRUN {i + 1} \\n\\n(A) Top-DT Performance Measures \\n\\n')\n",
    "        f.write(\"Hyperparameters: \" + str(json.dump(best_hyperparameters, f)) + \"\\n\\n\")\n",
    "        f.write('\\n(B) Confusion Matrix: \\n')\n",
    "        f.write(str(confusion_matrix(y_test_set, y_predict)))\n",
    "        f.write('\\n\\n(C - D) Classification Report: \\n')\n",
    "        f.write(str(classification_report(y_test_set, y_predict)))\n",
    "        f.write('\\n(C - D) Accuracy Score: ')\n",
    "        f.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base-MLP\n",
    "=> a Multi-Layered Perceptron with 2 hidden layers of 100+100 neurons, sigmoid/logistic as activation function, stochastic gradient descent, and default values for the rest of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation = 'logistic' => sigmoid function\n",
    "# solver = 'sgd' => stochastic gradient descent\n",
    "# default max_iter = 200 (number of epochs)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd')\n",
    "mlp.fit(X_train_set, y_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = mlp.predict(X_test_set)\n",
    "print(X_test_set)\n",
    "print(\"Predicted output: \", le.inverse_transform(y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base-MLP Performance Measures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abalone-performance.txt', 'a') as f:\n",
    "    f.write('\\n\\n\\n**********************************************************\\n\\n(A) Base-MLP Performance Measures \\n\\n')\n",
    "    f.write('Hyperparameters: \\n \\t- hidden_layer_sizes: (100, 100) \\n\\t- activation: logistic \\n\\t- solver: sgd \\n\\t- max_iter = 200 \\n\\t- shuffle = true \\n\\n')\n",
    "    f.write('\\n(B) Confusion Matrix: \\n')\n",
    "    f.write(str(confusion_matrix(y_test_set, y_predict)))\n",
    "    f.write('\\n\\n(C - D) Classification Report: \\n')\n",
    "    f.write(str(classification_report(y_test_set, y_predict)))\n",
    "    f.write('\\n(C - D) Accuracy Score: ')\n",
    "    f.write(str(accuracy_score(y_test_set, y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the Base-MLP code above 5 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mlp_accuracy_scores = []\n",
    "base_mlp_f1_scores_macro_avgs = []\n",
    "base_mlp_f1_scores_weighted_avgs = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    # activation = 'logistic' => sigmoid function\n",
    "    # solver = 'sgd' => stochastic gradient descent\n",
    "    # default max_iter = 200 (number of epochs)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd')\n",
    "    mlp.fit(X_train_set, y_train_set)\n",
    "    \n",
    "    y_predict = dtc_top_dt_best_params.predict(X_test_set)\n",
    "\n",
    "    print(X_test_set)\n",
    "    print(\"Predicted output: \", le.inverse_transform(y_predict))\n",
    "\n",
    "    accuracy = accuracy_score(y_test_set, y_predict)\n",
    "    base_mlp_accuracy_scores.append(accuracy)\n",
    "    \n",
    "    f1_score_macro_avg = f1_score(y_test_set, y_predict, average='macro')\n",
    "    base_mlp_f1_scores_macro_avgs.append(f1_score_macro_avg)\n",
    "    \n",
    "    f1_score_weighted_avg = f1_score(y_test_set, y_predict, average='weighted')\n",
    "    base_mlp_f1_scores_weighted_avgs.append(f1_score_weighted_avg)\n",
    "    \n",
    "    print(X_test_set)\n",
    "    print(\"Predicted output: \", le.inverse_transform(y_predict))\n",
    "    \n",
    "    with open('abalone-performance_5_runs.txt', 'a') as f:\n",
    "        f.write(f'\\n\\n\\n**********************************************************\\n\\nRUN {i + 1} \\n\\n(A) Base-MLP Performance Measures \\n\\n')\n",
    "        f.write('Hyperparameters: \\n \\t- hidden_layer_sizes: (100, 100) \\n\\t- activation: logistic \\n\\t- solver: sgd \\n\\t- max_iter = 200 \\n\\t- shuffle = true \\n\\n')\n",
    "        f.write('\\n(B) Confusion Matrix: \\n')\n",
    "        f.write(str(confusion_matrix(y_test_set, y_predict)))\n",
    "        f.write('\\n\\n(C - D) Classification Report: \\n')\n",
    "        f.write(str(classification_report(y_test_set, y_predict)))\n",
    "        f.write('\\n(C - D) Accuracy Score: ')\n",
    "        f.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-MLP\n",
    "=> a Multi-Layered Perceptron found using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_top = MLPClassifier()\n",
    "\n",
    "\n",
    "# define hyperparameters to test for best model\n",
    "hyperparameters = {\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'hidden_layer_sizes': [(100, 150), (200, 200, 200)],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'max_iter': [1500],\n",
    "}\n",
    "\n",
    "# use grid search to find best hyperparameters\n",
    "gs = GridSearchCV(mlp_top, hyperparameters, verbose=1)\n",
    "gs.fit(X_train_set, y_train_set)\n",
    "best_hyperparameters = gs.best_params_\n",
    "\n",
    "print('Best hyperparameters:\\n', best_hyperparameters)\n",
    "\n",
    "# create decision tree model with best hyperparameters\n",
    "mlp_top_best_params = MLPClassifier(**best_hyperparameters)\n",
    "\n",
    "# train model with best hyperparameters\n",
    "mlp_top_best_params.fit(X_train_set, y_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = mlp_top_best_params.predict(X_test_set)\n",
    "\n",
    "print(X_test_set)\n",
    "print(\"Predicted output: \", le.inverse_transform(y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-MLP Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abalone-performance.txt', 'a') as f:\n",
    "    f.write('\\n\\n\\n**********************************************************\\n\\n(A) Top-MLP Performance Measures \\n\\n')\n",
    "    f.write(\"Hyperparameters: \" + str(json.dump(best_hyperparameters, f)) + \"\\n\\n\")\n",
    "    f.write('\\n(B) Confusion Matrix: \\n')\n",
    "    f.write(str(confusion_matrix(y_test_set, y_predict)))\n",
    "    f.write('\\n\\n(C - D) Classification Report: \\n')\n",
    "    f.write(str(classification_report(y_test_set, y_predict)))\n",
    "    f.write('\\n(C - D) Accuracy Score: ')\n",
    "    f.write(str(accuracy_score(y_test_set, y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the Top-MLP code above 5 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mlp_accuracy_scores = []\n",
    "top_mlp_f1_scores_macro_avgs = []\n",
    "top_mlp_f1_scores_weighted_avgs = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    mlp_top = MLPClassifier()\n",
    "\n",
    "\n",
    "    # define hyperparameters to test for best model\n",
    "    hyperparameters = {\n",
    "        'activation': ['logistic', 'tanh', 'relu'],\n",
    "        'hidden_layer_sizes': [(100, 150), (200, 200, 200)],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'max_iter': [1500],\n",
    "    }\n",
    "\n",
    "    # use grid search to find best hyperparameters\n",
    "    gs = GridSearchCV(mlp_top, hyperparameters, verbose=1)\n",
    "    gs.fit(X_train_set, y_train_set)\n",
    "    best_hyperparameters = gs.best_params_\n",
    "\n",
    "    print('Best hyperparameters:\\n', best_hyperparameters)\n",
    "\n",
    "    # create decision tree model with best hyperparameters\n",
    "    mlp_top_best_params = MLPClassifier(**best_hyperparameters)\n",
    "\n",
    "    # train model with best hyperparameters\n",
    "    mlp_top_best_params.fit(X_train_set, y_train_set)\n",
    "        \n",
    "    y_predict = dtc_top_dt_best_params.predict(X_test_set)\n",
    "\n",
    "    print(X_test_set)\n",
    "    print(\"Predicted output: \", le.inverse_transform(y_predict))\n",
    "\n",
    "    accuracy = accuracy_score(y_test_set, y_predict)\n",
    "    top_mlp_accuracy_scores.append(accuracy)\n",
    "    \n",
    "    f1_score_macro_avg = f1_score(y_test_set, y_predict, average='macro')\n",
    "    top_mlp_f1_scores_macro_avgs.append(f1_score_macro_avg)\n",
    "    \n",
    "    f1_score_weighted_avg = f1_score(y_test_set, y_predict, average='weighted')\n",
    "    top_mlp_f1_scores_weighted_avgs.append(f1_score_weighted_avg)\n",
    "    \n",
    "    print(X_test_set)\n",
    "    print(\"Predicted output: \", le.inverse_transform(y_predict))\n",
    "    \n",
    "    with open('penguin-performance_5_runs.txt', 'a') as f:\n",
    "        f.write(f'\\n\\n\\n**********************************************************\\n\\nRUN {i + 1} \\n\\n(A) Top-MLP Performance Measures \\n\\n')\n",
    "        f.write(\"Hyperparameters: \" + str(json.dump(best_hyperparameters, f)) + \"\\n\\n\")\n",
    "        f.write('\\n(B) Confusion Matrix: \\n')\n",
    "        f.write(str(confusion_matrix(y_test_set, y_predict)))\n",
    "        f.write('\\n\\n(C - D) Classification Report: \\n')\n",
    "        f.write(str(classification_report(y_test_set, y_predict)))\n",
    "        f.write('\\n(C - D) Accuracy Score: ')\n",
    "        f.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Overall Averages of the 5 Runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abalone-performance_5_runs.txt', 'a') as f:\n",
    "        f.write(f'\\n\\n\\n**********************************************************\\n\\nOverall Performance Measure Averages of the 5 Runs \\n\\n')\n",
    "       \n",
    "        f.write('\\n\\n\\n*** Base-DT ***')\n",
    "        \n",
    "        f.write('\\nBase-DT Average Accuracy Score: ' + str(np.mean(base_dt_accuracy_scores)))\n",
    "        f.write('\\nBase-DT Average Accuracy Score Standard Deviation: ' + str(np.std(base_dt_accuracy_scores)))\n",
    "        f.write('\\nBase-DT Average Accuracy Score Variance: ' + str(np.var(base_dt_accuracy_scores)))\n",
    "        \n",
    "        f.write('\\n\\nBase-DT Average F1 Score (Macro): ' + str(np.mean(base_dt_f1_scores_macro_avgs)))\n",
    "        f.write('\\nBase-DT Average F1 Score (Macro) Standard Deviation: ' + str(np.std(base_dt_f1_scores_macro_avgs)))\n",
    "        f.write('\\nBase-DT Average F1 Score (Macro) Variance: ' + str(np.var(base_dt_f1_scores_macro_avgs)))\n",
    "        \n",
    "        \n",
    "        f.write('\\n\\nBase-DT Average F1 Score (Weighted): ' + str(np.mean(base_dt_f1_scores_weighted_avgs)))\n",
    "        f.write('\\nBase-DT Average F1 Score (Weighted) Standard Deviation: ' + str(np.std(base_dt_f1_scores_weighted_avgs)))\n",
    "        f.write('\\nBase-DT Average F1 Score (Weighted) Variance: ' + str(np.var(base_dt_f1_scores_weighted_avgs)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        f.write('\\n\\n\\n*** Top-DT ***')\n",
    "        \n",
    "        f.write('\\n\\n\\nTop-DT Average Accuracy Score: ' + str(np.mean(top_dt_accuracy_scores)))\n",
    "        f.write('\\nTop-DT Average Accuracy Score Standard Deviation: ' + str(np.std(top_dt_accuracy_scores)))\n",
    "        f.write('\\nTop-DT Average Accuracy Score Variance: ' + str(np.var(top_dt_accuracy_scores)))\n",
    "        \n",
    "        f.write('\\n\\nTop-DT Average F1 Score (Macro): ' + str(np.mean(top_dt_f1_scores_macro_avgs)))\n",
    "        f.write('\\nTop-DT Average F1 Score (Macro) Standard Deviation: ' + str(np.std(top_dt_f1_scores_macro_avgs)))\n",
    "        f.write('\\nTop-DT Average F1 Score (Macro) Variance: ' + str(np.var(top_dt_f1_scores_macro_avgs)))\n",
    "        \n",
    "        f.write('\\n\\nTop-DT Average F1 Score (Weighted): ' + str(np.mean(top_dt_f1_scores_weighted_avgs)))\n",
    "        f.write('\\nTop-DT Average F1 Score (Weighted) Standard Deviation: ' + str(np.std(top_dt_f1_scores_weighted_avgs)))\n",
    "        f.write('\\nTop-DT Average F1 Score (Weighted) Variance: ' + str(np.var(top_dt_f1_scores_weighted_avgs)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        f.write('\\n\\n\\n*** Base-MLP ***')\n",
    "        \n",
    "        f.write('\\n\\n\\nBase-MLP Average Accuracy Score: ' + str(np.mean(base_mlp_accuracy_scores)))\n",
    "        f.write('\\nBase-MLP Average Accuracy Score Standard Deviation: ' + str(np.std(base_mlp_accuracy_scores)))\n",
    "        f.write('\\nBase-MLP Average Accuracy Score Variance: ' + str(np.var(base_mlp_accuracy_scores)))\n",
    "        \n",
    "        f.write('\\n\\nBase-MLP Average F1 Score (Macro): ' + str(np.mean(base_mlp_f1_scores_macro_avgs)))\n",
    "        f.write('\\nBase-MLP Average F1 Score (Macro) Standard Deviation: ' + str(np.std(base_mlp_f1_scores_macro_avgs)))\n",
    "        f.write('\\nBase-MLP Average F1 Score (Macro) Variance: ' + str(np.var(base_mlp_f1_scores_macro_avgs)))\n",
    "        \n",
    "        f.write('\\n\\nBase-MLP Average F1 Score (Weighted): ' + str(np.mean(base_mlp_f1_scores_weighted_avgs)))\n",
    "        f.write('\\nBase-MLP Average F1 Score (Weighted) Standard Deviation: ' + str(np.std(base_mlp_f1_scores_weighted_avgs)))\n",
    "        f.write('\\nBase-MLP Average F1 Score (Weighted) Variance: ' + str(np.var(base_mlp_f1_scores_weighted_avgs)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        f.write('\\n\\n\\n*** Top-MLP ***')\n",
    "        \n",
    "        f.write('\\n\\n\\nTop-MLP Average Accuracy Score: ' + str(np.mean(top_mlp_accuracy_scores)))\n",
    "        f.write('\\nTop-MLP Average Accuracy Score Standard Deviation: ' + str(np.std(top_mlp_accuracy_scores)))\n",
    "        f.write('\\nTop-MLP Average Accuracy Score Variance: ' + str(np.var(top_mlp_accuracy_scores)))\n",
    "        \n",
    "        f.write('\\n\\nTop-MLP Average F1 Score (Macro): ' + str(np.mean(top_mlp_f1_scores_macro_avgs)))\n",
    "        f.write('\\nTop-MLP Average F1 Score (Macro) Standard Deviation: ' + str(np.std(top_mlp_f1_scores_macro_avgs)))\n",
    "        f.write('\\nTop-MLP Average F1 Score (Macro) Variance: ' + str(np.var(top_mlp_f1_scores_macro_avgs)))\n",
    "        \n",
    "        f.write('\\n\\nTop-MLP Average F1 Score (Weighted): ' + str(np.mean(top_mlp_f1_scores_weighted_avgs)))\n",
    "        f.write('\\nTop-MLP Average F1 Score (Weighted) Standard Deviation: ' + str(np.std(top_mlp_f1_scores_weighted_avgs)))\n",
    "        f.write('\\nTop-MLP Average F1 Score (Weighted) Variance: ' + str(np.var(top_mlp_f1_scores_weighted_avgs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
